{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R7fXpYUx-oYh"},"outputs":[],"source":["!pip install -qU /content/drive/MyDrive/Final_Projects\n","!pip install -qU langchain-groq\n","!pip install -q flask flask-cors\n","!pip install -q pyngrok\n","!pip install -q flask-ngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9886,"status":"ok","timestamp":1733900597507,"user":{"displayName":"Bakhshial Hajano","userId":"06224582539046210768"},"user_tz":-300},"id":"xq28olyZ-x1o","outputId":"7e33cabd-8b38-40e7-d071-10fb4d6ebb69"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n"]}],"source":["!apt-get update\n","!apt-get install -y tesseract-ocr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_j1-jpgFdBB"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Final_Projects')\n","\n","from data_manager import DataManager"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcgMCsPw_P0b","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d7bd81e-571d-4a6a-9b6f-d8e6868847c0"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-10-f7ca35fa36c8>:67: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Ngrok Tunnel URL: NgrokTunnel: \"https://ab94-34-125-165-121.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:5000\n"," * Running on http://172.28.0.12:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:26:51] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:26:52] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:27:14] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:27:15] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:27:35] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:27:36] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:27:50] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:27:51] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:28:16] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:28:17] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:32:49] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:32:50] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:33:04] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:33:05] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:36:46] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:36:47] \"POST /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:41:27] \"OPTIONS /api/query HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Dec/2024 07:41:28] \"POST /api/query HTTP/1.1\" 200 -\n"]}],"source":["from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","from langchain.chains import RetrievalQA\n","from langchain.memory import ConversationBufferMemory\n","from langchain_groq import ChatGroq\n","from data_manager import DataManager\n","from pyngrok import ngrok\n","from flask_ngrok import run_with_ngrok\n","import os\n","import json\n","\n","import os\n","os.environ['GROQ_API_KEY'] = Ues api key\n","\n","os.environ['lamaaccess'] = use api key\n","\n","# Ensure GROQ_API_KEY is set\n","if 'GROQ_API_KEY' not in os.environ:\n","    raise EnvironmentError(\"GROQ_API_KEY is not set. Please set it as an environment variable.\")\n","\n","# Ngrok authentication token\n","ngrok.set_auth_token(use api key)\n","\n","# Initialize Flask app\n","app = Flask(__name__)\n","CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n","#run_with_ngrok(app)\n","\n","# Paths and setup\n","folder_path=\"/content/drive/MyDrive/Final_Project1/my_data\"\n","#folder_path = \"/content/drive/MyDrive/Final_Projects/Cyber_Securitydataset\"\n","persist_dir = \"/content/drive/MyDrive/Final_Project/Chromdb\"\n","responses_path = \"/content/drive/MyDrive/Final_Project/responses.json\"\n","urls = [\n","    \"https://www.geeksforgeeks.org/cyber-security-types-and-importance/\",\n","    \"https://en.wikipedia.org/wiki/Computer_security\",\n","]\n","\n","# Initialize DataManager\n","data_manager = DataManager(persist_dir)\n","try:\n","\n","    # Extract and clean data\n","    folder_data = data_manager.extract_data_from_folder(folder_path)\n","    website_data = data_manager.extract_data_from_websites(urls)\n","    combined_data = folder_data + website_data\n","    # Chunk and embed data\n","    chunks = data_manager.chunk_data(combined_data)\n","    data_manager.embed_and_store_data(chunks)\n","    retrieval_chain = data_manager.get_retrieval_chain()\n","except Exception as e:\n","    print(f\"Error initializing DataManager: {e}\")\n","    raise\n","\n","# Initialize ChatGroq model\n","try:\n","    chat_model = ChatGroq(\n","        temperature=0,\n","        groq_api_key=os.environ['GROQ_API_KEY'],\n","        model=\"llama3-8b-8192\"\n","    )\n","except Exception as e:\n","    print(f\"Error initializing ChatGroq: {e}\")\n","    raise\n","\n","# Initialize memory for context retention\n","memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","\n","# Create the RetrievalQA chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=chat_model,\n","    chain_type=\"stuff\",\n","    retriever=retrieval_chain,\n","    return_source_documents=True,\n",")\n","\n","# Function to save responses\n","def save_responses_to_file(response, file_path):\n","    try:\n","        if not os.path.exists(file_path):\n","            with open(file_path, \"w\") as file:\n","                json.dump([], file)\n","\n","        with open(file_path, \"r+\") as file:\n","            responses = json.load(file)\n","            responses.append(response)\n","            file.seek(0)\n","            json.dump(responses, file, indent=4)\n","    except Exception as e:\n","        print(f\"Error saving responses to file: {e}\")\n","\n","# Function to process responses\n","def process_llm_response(query, llm_response, return_sources=False):\n","    try:\n","        result = llm_response['result']\n","        sources = [\n","            source.metadata.get('source', 'Unknown') for source in llm_response.get(\"source_documents\", [])\n","        ]\n","\n","        response_entry = {\n","            \"query\": query,\n","            \"result\": result,\n","            \"sources\": sources\n","        }\n","        save_responses_to_file(response_entry, responses_path)\n","\n","        if return_sources:\n","            return result, sources\n","        return result\n","    except Exception as e:\n","        print(f\"Error processing LLM response: {e}\")\n","        return \"An error occurred while processing the response.\"\n","\n","# Flask API endpoint\n","@app.route('/api/query', methods=['POST'])\n","def query():\n","    try:\n","        data = request.json\n","        question = data.get(\"question\", \"\")\n","\n","        # Ensure memory manually retains context\n","        memory.chat_memory.add_user_message(question)\n","\n","        # Invoke the QA chain\n","        llm_response = qa_chain.invoke({\"query\": question})\n","\n","        # Add AI response to memory for context retention\n","        memory.chat_memory.add_ai_message(llm_response['result'])\n","\n","        # Process the response\n","        response = process_llm_response(question, llm_response)\n","        return jsonify({\"answer\": response})\n","    except Exception as e:\n","        print(f\"Error in /api/query endpoint: {e}\")\n","        return jsonify({\"error\": \"Internal Server Error\"}), 500\n","\n","@app.route('/api/sources', methods=['POST'])\n","def sources():\n","    try:\n","        data = request.json\n","        question = data.get(\"question\", \"\")\n","\n","        # Ensure memory manually retains context\n","        memory.chat_memory.add_user_message(question)\n","\n","        # Invoke the QA chain\n","        llm_response = qa_chain.invoke({\"query\": question})\n","\n","        # Add AI response to memory for context retention\n","        memory.chat_memory.add_ai_message(llm_response['result'])\n","\n","        # Process the response and return sources\n","        _, sources = process_llm_response(question, llm_response, return_sources=True)\n","        return jsonify({\"sources\": sources})\n","    except Exception as e:\n","        print(f\"Error in /api/sources endpoint: {e}\")\n","        return jsonify({\"error\": \"Internal Server Error\"}), 500\n","\n","# Run Flask app with ngrok tunnel\n","if __name__ == \"__main__\":\n","    try:\n","        # Start ngrok tunnel\n","        public_url = ngrok.connect(5000)\n","        print(f\"Ngrok Tunnel URL: {public_url}\")\n","\n","        # Start Flask server\n","        app.run(host=\"0.0.0.0\", port=5000)\n","    except Exception as e:\n","        print(f\"Error starting Flask app with ngrok: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MtLH1SX_VuV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"19yc8vnpFZP6WawmwuzitaDOBhaZkOFsw","authorship_tag":"ABX9TyPi6dCnhSzzEhkq5qAAKIju"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}